{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3e0a511",
   "metadata": {},
   "source": [
    "## Data Split: Current and Future data\n",
    "\n",
    "References:\n",
    "- https://github.com/andrewwlong/diabetes_readmission/blob/master/diabetes_project.ipynb\n",
    "- https://github.com/iterative/course-ds-base/blob/step-1-organize-ml-project_SOLUTION/notebooks/step-1-organize-ml-project.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3134f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62906abc",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "17c4c8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/stages/download_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/stages/download_data.py\n",
    "\n",
    "import os\n",
    "import wget\n",
    "import yaml\n",
    "import argparse\n",
    "\n",
    "from src.utils.logs import get_logger\n",
    "\n",
    "def download_data(config_path):\n",
    "    \n",
    "    with open(config_path) as conf_file:\n",
    "        config = yaml.safe_load(conf_file)\n",
    "\n",
    "    logger = get_logger('DOWNLOAD DATA', log_level=config['base']['log_level'])\n",
    "\n",
    "    url = config['data_load']['source']\n",
    "        \n",
    "    logger.info(f'Downloading from {url}')\n",
    "    local_folder = config['data_load']['local_folder']\n",
    "    local_filename = os.path.join(local_folder, config['data_load']['local_name'])\n",
    "\n",
    "\n",
    "    # Create the folder if it does not exist\n",
    "    os.makedirs(local_folder, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    # Download the file\n",
    "    wget.download(url, local_filename)\n",
    "    logger.info(f\"Downloaded {local_filename}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    args_parser = argparse.ArgumentParser()\n",
    "    args_parser.add_argument('--config', dest='config', required=True)\n",
    "    args = args_parser.parse_args()\n",
    "\n",
    "    download_data(config_path=args.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9313500-d96f-4edd-9392-7f112675f769",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "830e10f6-a3be-4914-8eba-f2981684815b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/stages/load_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/stages/load_data.py\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "from src.utils.utils import calc_prevalence\n",
    "from src.utils.logs import get_logger\n",
    "\n",
    "def load_data(config_path):\n",
    "\n",
    "    with open(config_path) as conf_file:\n",
    "        config = yaml.safe_load(conf_file)\n",
    "\n",
    "    logger = get_logger('LOAD DATA', log_level=config['base']['log_level'])\n",
    "\n",
    "    local_folder = config['data_load']['local_folder']\n",
    "    local_filename = os.path.join(local_folder, config['data_load']['local_name'])\n",
    "\n",
    "    # load the csv file\n",
    "    df = pd.read_csv(local_filename)\n",
    "    #Here we will label if a patient is likely to be re-admitted within 30 days of discharge.\n",
    "    df['OUTPUT_LABEL'] = (df.readmitted == '<30').astype('int')\n",
    "    logger.info('Prevalence:%.3f'%calc_prevalence(df['OUTPUT_LABEL'].values))\n",
    "\n",
    "    # shuffle the samples\n",
    "    df = df.sample(n = len(df), random_state = 42)\n",
    "    df = df.reset_index(drop = True)\n",
    "    \n",
    "    # Save 30% of the data as future data \n",
    "    df_future=df.sample(frac=0.30,random_state=42)\n",
    "    df_current = df.drop(df_future.index)\n",
    "    \n",
    "    logger.info('Split size current: %.3f'%(len(df_current)/len(df)))\n",
    "    logger.info('Split size future: %.3f'%(len(df_future)/len(df)))\n",
    "\n",
    "    logger.info('Prevalence current:%.3f'%calc_prevalence(df_current['OUTPUT_LABEL'].values))\n",
    "    logger.info('Prevalence future:%.3f'%calc_prevalence(df_future['OUTPUT_LABEL'].values))\n",
    "    \n",
    "    # save raw data\n",
    "    \n",
    "    df_current.to_csv(config['data_load']['dataset_csv'], index=False)\n",
    "    df_future.to_csv(config['data_load']['future_csv'], index=False)\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    args_parser = argparse.ArgumentParser()\n",
    "    args_parser.add_argument('--config', dest='config', required=True)\n",
    "    args = args_parser.parse_args()\n",
    "\n",
    "    load_data(config_path=args.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d5bb1d-a91c-437c-aafc-903cd433c054",
   "metadata": {},
   "source": [
    "## Append new data for continuous learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7902d93d-0fd6-43d7-ab6d-94ae49e1b2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/stages/append_new_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/stages/append_new_data.py\n",
    "\n",
    "import wget\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import yaml\n",
    "import shutil\n",
    "\n",
    "from src.utils.logs import get_logger\n",
    "\n",
    "def append_new_data(config_path):\n",
    "\n",
    "    with open(config_path) as conf_file:\n",
    "        config = yaml.safe_load(conf_file)\n",
    "\n",
    "    logger = get_logger('APPEND NEW DATA', log_level=config['base']['log_level'])\n",
    "\n",
    "    if not config['base']['continuous_learning']:\n",
    "        logger.info(\"Continuous learning NOT enabled\")\n",
    "        # just rename the output\n",
    "        \n",
    "        shutil.copy(config['data_load']['dataset_csv'], config['data_load']['appended_dataset_csv'])\n",
    "        return\n",
    "\n",
    "    logger.info(\"continuous learning\")\n",
    "    new_data_url = config['data_load']['source_new']\n",
    "    local_filename = config['data_load']['new_data']\n",
    "    \n",
    "    \n",
    "    # Download the file\n",
    "    logger.info(f'Downloading from {new_data_url}')\n",
    "    wget.download(new_data_url, local_filename)\n",
    "    logger.info(f\"Downloaded {local_filename}\")\n",
    "\n",
    "    ## Append to the current data\n",
    "    current_csv_name = config['data_load']['dataset_csv']\n",
    "    df = pd.read_csv(current_csv_name)\n",
    "    logger.info(f\"Shape before adding new data {df.shape}\")\n",
    "    \n",
    "    new_df = pd.read_csv(config['data_load']['new_data'])\n",
    "    logger.info(f\"Shape of new data {new_df.shape}\")\n",
    "\n",
    "    ## combine the two dataframe and save it\n",
    "\n",
    "    result_df = pd.concat([df, new_df], ignore_index=True)\n",
    "    \n",
    "    logger.info(f\"Shape after adding new data {result_df.shape}\")\n",
    "\n",
    "    current_csv_name = config['data_load']['appended_dataset_csv']\n",
    "    result_df.to_csv(current_csv_name, index=False)\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    args_parser = argparse.ArgumentParser()\n",
    "    args_parser.add_argument('--config', dest='config', required=True)\n",
    "    args = args_parser.parse_args()\n",
    "\n",
    "    append_new_data(config_path=args.config)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc200e13-57af-41a4-b673-2bf1e08d23c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4d63caf",
   "metadata": {},
   "source": [
    "## Features engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36a82a42-0d74-4bf5-8703-b6609e7750cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ..\\src\\stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad772fb9-2c14-4933-95eb-1a856f3d1218",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ..\\data\\processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e91f8212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/stages/featurize.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/stages/featurize.py\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from typing import Text\n",
    "import yaml\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "from src.utils.logs import get_logger\n",
    "\n",
    "def featurize(config_path, train=False):\n",
    "\n",
    "    with open(config_path) as conf_file:\n",
    "        config = yaml.safe_load(conf_file)\n",
    "\n",
    "    logger = get_logger('FEATURE ENG', log_level=config['base']['log_level'])\n",
    "\n",
    "    # Create the folder if it does not exist\n",
    "    os.makedirs(config['featurize']['folder_processed'], exist_ok=True)\n",
    "    \n",
    "\n",
    "    #df = pd.read_csv(path)\n",
    "    if train:\n",
    "        logger.info('Using training data, fittin encoder')\n",
    "        df = pd.read_csv(config['data_load']['dataset_csv'])\n",
    "    else:\n",
    "        logger.info('new data, use previously fitted encoder')\n",
    "        df = pd.read_csv(config['data_load']['inference_dataset_csv'])\n",
    "    \n",
    "\n",
    "    # replace ? with nan\n",
    "    df = df.replace('?',np.nan)\n",
    "    \n",
    "    cols_num = ['time_in_hospital','num_lab_procedures', 'num_procedures', 'num_medications',\n",
    "           'number_outpatient', 'number_emergency', 'number_inpatient','number_diagnoses']\n",
    "\n",
    "    cols_cat = ['race', 'gender', \n",
    "           'max_glu_serum', 'A1Cresult',\n",
    "           'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide',\n",
    "           'glimepiride', 'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide',\n",
    "           'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone',\n",
    "           'tolazamide', 'insulin',\n",
    "           'glyburide-metformin', 'glipizide-metformin',\n",
    "           'glimepiride-pioglitazone', 'metformin-rosiglitazone',\n",
    "           'metformin-pioglitazone', 'change', 'diabetesMed','payer_code']\n",
    "    \n",
    "    \n",
    "\n",
    "    # handle missing values\n",
    "    logger.info('Handling missing values')\n",
    "    df['race'] = df['race'].fillna('UNK')\n",
    "    df['payer_code'] = df['payer_code'].fillna('UNK')\n",
    "    df['medical_specialty'] = df['medical_specialty'].fillna('UNK')\n",
    "\n",
    "    df['max_glu_serum'] = df['max_glu_serum'].fillna('UNK')\n",
    "    df['A1Cresult'] = df['A1Cresult'].fillna('UNK')\n",
    "\n",
    "    # Bucket Medical Speciality\n",
    "    top_10 = ['UNK','InternalMedicine','Emergency/Trauma',\\\n",
    "              'Family/GeneralPractice', 'Cardiology','Surgery-General' ,\\\n",
    "              'Nephrology','Orthopedics',\\\n",
    "              'Orthopedics-Reconstructive','Radiologist']\n",
    "\n",
    "    # make a new column with duplicated data\n",
    "    df['med_spec'] = df['medical_specialty'].copy()\n",
    "\n",
    "    # replace all specialties not in top 10 with 'Other' category\n",
    "    df.loc[~df.med_spec.isin(top_10),'med_spec'] = 'Other'\n",
    "\n",
    "\n",
    "    # categorical numeric data into string type, to use with get_dummies\n",
    "    cols_cat_num = ['admission_type_id', 'discharge_disposition_id', 'admission_source_id']\n",
    "    df[cols_cat_num] = df[cols_cat_num].astype('str')\n",
    "\n",
    "\n",
    "    cols_to_encode = cols_cat + cols_cat_num + ['med_spec']\n",
    "    encoder_path = config['featurize']['encoder_path']\n",
    "\n",
    "    if train:\n",
    "        # Initialize OneHotEncoder with handle_unknown set to 'ignore'\n",
    "        encoder = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "        # Fit the encoder on your data\n",
    "        logger.info('fitting encoder')\n",
    "        encoder.fit(df[cols_to_encode])\n",
    "\n",
    "        # Serialize the encoder to a file\n",
    "        joblib.dump(encoder, encoder_path)\n",
    "        logger.info('Encoder object serialized')\n",
    "    \n",
    "    else:\n",
    "        # Deserialize the encoder from the file\n",
    "        logger.info('Loadign encoder object')\n",
    "        encoder = joblib.load(encoder_path)\n",
    "\n",
    "    # Use the encoder to transform new data\n",
    "    new_encoded_array = encoder.transform(df[cols_to_encode])\n",
    "    new_encoded_df = pd.DataFrame(new_encoded_array, columns=encoder.get_feature_names_out(cols_to_encode))\n",
    "\n",
    "    df = pd.concat([df,new_encoded_df], axis = 1)\n",
    "\n",
    "    cols_all_cat = list(new_encoded_df.columns)\n",
    "\n",
    "    age_id = {'[0-10)':0, \n",
    "              '[10-20)':10, \n",
    "              '[20-30)':20, \n",
    "              '[30-40)':30, \n",
    "              '[40-50)':40, \n",
    "              '[50-60)':50,\n",
    "              '[60-70)':60, \n",
    "              '[70-80)':70, \n",
    "              '[80-90)':80, \n",
    "              '[90-100)':90}\n",
    "    df['age_group'] = df.age.replace(age_id)\n",
    "\n",
    "    df['has_weight'] = df.weight.notnull().astype('int')\n",
    "    cols_extra = ['age_group','has_weight']\n",
    "\n",
    "\n",
    "    logger.info(f'Total number of features: {len(cols_num + cols_all_cat + cols_extra)}')\n",
    "    logger.info(f'Numerical Features:  {len(cols_num)}')\n",
    "    logger.info(f'Categorical Features: {len(cols_all_cat)}')\n",
    "    logger.info(f'Extra features: {len(cols_extra)}')\n",
    "    logger.info(f'Data shape:{df.shape}')\n",
    "    \n",
    "    col2use = cols_num + cols_all_cat + cols_extra\n",
    "    featured_dataset = df[col2use + ['OUTPUT_LABEL']]\n",
    "    features_path = config['featurize']['features_path']\n",
    "    #df_data.to_csv(\"../data/processed/featured.csv\")\n",
    "    featured_dataset.to_csv(features_path, index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    args_parser = argparse.ArgumentParser()\n",
    "    args_parser.add_argument('--config', dest='config', required=True)\n",
    "    args_parser.add_argument('--train', action='store_true', help='Fit and transform encoder')\n",
    "    args = args_parser.parse_args()\n",
    "\n",
    "    featurize(config_path=args.config, train=args.train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "925444d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src.stages.featurize import featurize\n",
    "\n",
    "#featurize('../params.yaml', fit_encoder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7fd400f-5d77-45df-b685-db5c359adc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d905a29d-76de-4572-81db-cce5fadd41e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D has no label.\n",
      " Volume Serial Number is A6CB-3452\n",
      "\n",
      " Directory of D:\\ik_capstone_v2\\notebooks\n",
      "\n",
      "30-07-2024  08:16    <DIR>          .\n",
      "30-07-2024  08:16    <DIR>          ..\n",
      "30-07-2024  07:42    <DIR>          .ipynb_checkpoints\n",
      "30-07-2024  08:16            32,678 step-0-prototype.ipynb\n",
      "               1 File(s)         32,678 bytes\n",
      "               3 Dir(s)  273,763,733,504 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "919671f3-08db-41fa-a8a9-73415c00dd8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shint\\AppData\\Local\\Programs\\Python\\Python310\\python.exe: can't open file 'D:\\\\ik_capstone_v2\\\\notebooks\\\\src\\\\stages\\\\featurize.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python src\\stages\\featurize.py --config ..\\params.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2a8c22",
   "metadata": {},
   "source": [
    "## Building Training/Validation/Test Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a21a534",
   "metadata": {},
   "source": [
    "### Handling imbalance\n",
    "\n",
    "Typically, it is better to balance the data in some way to give the positives more weight. There are 3 strategies that are typically utilized: - sub-sample the more dominant class: use a random subset of the negatives - over-sample the imbalanced class: use the same positive samples multiple times - create synthetic positive data\n",
    "\n",
    "Usually, you will want to use the latter two methods if you only have a handful of positive cases. Since we have a few thousand positive cases, let's use the sub-sample approach. Here, we will create a balanced training data set that has 50% positive and 50% negative. You can also play with this ratio to see if you can get an improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72173881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7566f838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/stages/data_split.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/stages/data_split.py\n",
    "\n",
    "import argparse\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from src.utils.utils import calc_prevalence\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "from src.utils.logs import get_logger\n",
    "\n",
    "def data_split(config_path):\n",
    "\n",
    "    with open(config_path) as conf_file:\n",
    "        config = yaml.safe_load(conf_file)\n",
    "\n",
    "    logger = get_logger('DATA SPLIT', log_level=config['base']['log_level'])\n",
    "\n",
    "    df_data = pd.read_csv(config['featurize']['features_path'])\n",
    "\n",
    "\n",
    "    # Save 30% of the data as validation and test data \n",
    "    df_valid_test=df_data.sample(frac=0.30,random_state=42)\n",
    "    logger.info('Split size: %.3f'%(len(df_valid_test)/len(df_data)))\n",
    "\n",
    "    df_test = df_valid_test.sample(frac = 0.5, random_state = 42)\n",
    "    df_valid = df_valid_test.drop(df_test.index)\n",
    "\n",
    "    # use the rest of the data as training data\n",
    "    df_train_all=df_data.drop(df_valid_test.index)\n",
    "\n",
    "\n",
    "    # Fit the scaler using all training data\n",
    "    scaler  = StandardScaler()\n",
    "    #X_train_all = df_train_all[].values.astype('float32')\n",
    "    target_column=config['featurize']['target_column']\n",
    "    \n",
    "    X_train_all = df_train_all.drop(target_column, axis=1).values.astype('float32')\n",
    "    scaler.fit(X_train_all)\n",
    "    scalerfile = config['data_split']['scaler_path']\n",
    "    pickle.dump(scaler, open(scalerfile, 'wb'))\n",
    "\n",
    "    logger.info('Test prevalence(n = %d):%.3f'%(len(df_test),calc_prevalence(df_test.OUTPUT_LABEL.values)))\n",
    "    logger.info('Valid prevalence(n = %d):%.3f'%(len(df_valid),calc_prevalence(df_valid.OUTPUT_LABEL.values)))\n",
    "    logger.info('Train all prevalence(n = %d):%.3f'%(len(df_train_all), calc_prevalence(df_train_all.OUTPUT_LABEL.values)))\n",
    "    \n",
    "    # Handling imbalance\n",
    "    # use the sub-sample approach. Here, we will create a balanced training data set that has 50% positive and 50% negative.\n",
    "    # You can also play with this ratio to see if you can get an improvement.\n",
    "    \n",
    "    # split the training data into positive and negative\n",
    "    rows_pos = df_train_all.OUTPUT_LABEL == 1\n",
    "    df_train_pos = df_train_all.loc[rows_pos]\n",
    "    df_train_neg = df_train_all.loc[~rows_pos]\n",
    "\n",
    "    # merge the balanced data\n",
    "    df_train = pd.concat([df_train_pos, df_train_neg.sample(n = len(df_train_pos), random_state = 42)],axis = 0)\n",
    "\n",
    "    # shuffle the order of training samples \n",
    "    df_train = df_train.sample(n = len(df_train), random_state = 42).reset_index(drop = True)\n",
    "\n",
    "    logger.info('Train balanced prevalence(n = %d):%.3f'%(len(df_train), calc_prevalence(df_train.OUTPUT_LABEL.values)))\n",
    "\n",
    "\n",
    "    \n",
    "    train_csv_path = config['data_split']['trainset_path']\n",
    "    validset_path = config['data_split']['validset_path']\n",
    "    testset_path = config['data_split']['testset_path']\n",
    "    train_unbalanced_path = config['data_split']['train_unbalanced_path']\n",
    "    \n",
    "    df_train_all.to_csv(train_unbalanced_path, index=False)\n",
    "    df_train.to_csv(train_csv_path, index=False)\n",
    "    df_valid.to_csv(validset_path, index=False)\n",
    "    df_test.to_csv(testset_path, index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    args_parser = argparse.ArgumentParser()\n",
    "    args_parser.add_argument('--config', dest='config', required=True)\n",
    "    args = args_parser.parse_args()\n",
    "\n",
    "    data_split(config_path=args.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2374ea3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Split size: 0.300\n",
      "Test prevalence(n = 10686):0.110\n",
      "Valid prevalence(n = 10685):0.114\n",
      "Train all prevalence(n = 49865):0.113\n",
      "Train balanced prevalence(n = 11222):0.500\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from data_split import data_split\n",
    "\n",
    "data_split(\"../data/processed/featured.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506a1bd4",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0cb19b68-e77f-4e40-b4ca-cb9eae1120d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ..\\src\\train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3e44de31-07f5-47e6-951a-d03129a1a202",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ..\\models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bdde0563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/train/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/train/train.py\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from typing import Dict, Text\n",
    "import pickle\n",
    "\n",
    "from src.utils.logs import get_logger\n",
    "\n",
    "\n",
    "class UnsupportedClassifier(Exception):\n",
    "\n",
    "    def __init__(self, estimator_name):\n",
    "\n",
    "        self.msg = f'Unsupported estimator {estimator_name}'\n",
    "        super().__init__(self.msg)\n",
    "\n",
    "\n",
    "def get_supported_estimator() -> Dict:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        Dict: supported classifiers\n",
    "    \"\"\"\n",
    "\n",
    "    return {\n",
    "        'logreg': LogisticRegression,\n",
    "        'svm': SVC,\n",
    "        'knn': KNeighborsClassifier\n",
    "    }\n",
    "\n",
    "\n",
    "def train(df: pd.DataFrame, target_column: Text,\n",
    "          estimator_name: Text, scaler_path: Text, param_grid: Dict,  cv: int):\n",
    "    \"\"\"Train model.\n",
    "    Args:\n",
    "        df {pandas.DataFrame}: dataset\n",
    "        target_column {Text}: target column name\n",
    "        estimator_name {Text}: estimator name\n",
    "        param_grid {Dict}: grid parameters\n",
    "        cv {int}: cross-validation value\n",
    "    Returns:\n",
    "        trained model\n",
    "    \"\"\"\n",
    "\n",
    "    estimators = get_supported_estimator()\n",
    "\n",
    "    if estimator_name not in estimators.keys():\n",
    "        raise UnsupportedClassifier(estimator_name)\n",
    "\n",
    "    estimator = estimators[estimator_name]()\n",
    "    f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "    clf = GridSearchCV(estimator=estimator,\n",
    "                       param_grid=param_grid,\n",
    "                       cv=cv,\n",
    "                       verbose=1,\n",
    "                       scoring=f1_scorer)\n",
    "    # Get X and Y\n",
    "    y_train = df.loc[:, target_column].values.astype('int32')\n",
    "    X_train = df.drop(target_column, axis=1).values.astype('float32')\n",
    "\n",
    "\n",
    "    # load the scaler\n",
    "    scaler = pickle.load(open(scaler_path, 'rb'))\n",
    "\n",
    "    X_train_tf = scaler.transform(X_train)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "43b4486f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/stages/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/stages/train.py\n",
    "\n",
    "\n",
    "import argparse\n",
    "import joblib\n",
    "\n",
    "import pandas as pd\n",
    "from typing import Text\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "from src.train.train import train\n",
    "from src.utils.logs import get_logger\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(config_path: Text) -> None:\n",
    "\n",
    "\n",
    "    with open(config_path) as conf_file:\n",
    "        config = yaml.safe_load(conf_file)\n",
    "\n",
    "    logger = get_logger('TRAIN', log_level=config['base']['log_level'])\n",
    "\n",
    "    estimator_name = config['train']['estimator_name']\n",
    "\n",
    "    train_df = pd.read_csv(config['data_split']['trainset_path'])\n",
    "\n",
    "\n",
    "\n",
    "    model = train(\n",
    "        df=train_df,\n",
    "        target_column=config['featurize']['target_column'],\n",
    "        estimator_name=estimator_name,\n",
    "        scaler_path=config['data_split']['scaler_path'],\n",
    "        param_grid=config['train']['estimators'][estimator_name]['param_grid'],\n",
    "        cv=config['train']['cv']\n",
    "    )\n",
    "    \n",
    "    logger.info(f'Best score: {model.best_score_}')\n",
    "\n",
    "    \n",
    "    # Create the folder if it does not exist\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    logger.info('Save model')\n",
    "    models_path = config['train']['model_path']\n",
    "    joblib.dump(model, models_path)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    args_parser = argparse.ArgumentParser()\n",
    "    args_parser.add_argument('--config', dest='config', required=True)\n",
    "    args = args_parser.parse_args()\n",
    "\n",
    "    train_model(config_path=args.config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f216cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"D:\\IK_CAPSTONE_PROJECT\\src\\stages\\train.py\", line 8, in <module>\n",
      "    from src.train.train import train\n",
      "ModuleNotFoundError: No module named 'src'\n"
     ]
    }
   ],
   "source": [
    "!python ..\\src\\stages\\train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61edbdff-41e9-4725-8c26-f68bb5da191d",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c69ed973-d428-4991-8f00-87df2120bfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ..\\src\\report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd7986a2-6dfd-4831-add8-590291e3b381",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ..\\reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c585df97-034b-4bc4-9959-c1f3ce68e025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../src/report/visualize.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/report/visualize.py\n",
    "import itertools\n",
    "import matplotlib.colors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import List, Text\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm: np.array,\n",
    "                          target_names: List[Text],\n",
    "                          title: Text = 'Confusion matrix',\n",
    "                          cmap: matplotlib.colors.LinearSegmentedColormap = None,\n",
    "                          normalize: bool = True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "\n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c8d8395f-4741-4b6a-89e4-3e0e98a736ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/stages/evaluate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/stages/evaluate.py\n",
    "import argparse\n",
    "import joblib\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from typing import Text, Dict\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "from src.report.visualize import plot_confusion_matrix\n",
    "from src.utils.logs import get_logger\n",
    "\n",
    "def convert_to_labels(indexes, labels):\n",
    "    result = []\n",
    "    for i in indexes:\n",
    "        result.append(labels[i])\n",
    "    return result\n",
    "\n",
    "def write_confusion_matrix_data(y_true, predicted, labels, filename):\n",
    "    assert len(predicted) == len(y_true)\n",
    "    predicted_labels = convert_to_labels(predicted, labels)\n",
    "    true_labels = convert_to_labels(y_true, labels)\n",
    "    cf = pd.DataFrame(list(zip(true_labels, predicted_labels)), columns=[\"y_true\", \"predicted\"])\n",
    "    cf.to_csv(filename, index=False)\n",
    "\n",
    "def evaluate_model(config_path: Text) -> None:\n",
    "    \"\"\"Evaluate model.\n",
    "    Args:\n",
    "        config_path {Text}: path to config\n",
    "    \"\"\"\n",
    "\n",
    "    with open(config_path) as conf_file:\n",
    "        config = yaml.safe_load(conf_file)\n",
    "\n",
    "    logger = get_logger('EVALUATE', log_level=config['base']['log_level'])\n",
    "\n",
    "    logger.info('Load model')\n",
    "    model_path = config['train']['model_path']\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "    logger.info('Load test dataset')\n",
    "    test_df = pd.read_csv(config['data_split']['testset_path'])\n",
    "\n",
    "    logger.info('Evaluate (build report)')\n",
    "    target_column=config['featurize']['target_column']\n",
    "    y_test = test_df.loc[:, target_column].values\n",
    "    X_test = test_df.drop(target_column, axis=1).values\n",
    "\n",
    "    prediction = model.predict(X_test)\n",
    "    f1 = f1_score(y_true=y_test, y_pred=prediction, average='macro')\n",
    "\n",
    "    labels = ['Yes', 'No']\n",
    "    cm = confusion_matrix(prediction, y_test)\n",
    "    report = {\n",
    "        'f1': f1,\n",
    "        'cm': cm,\n",
    "        'actual': y_test,\n",
    "        'predicted': prediction\n",
    "    }\n",
    "\n",
    "    logger.info('Save metrics')\n",
    "    # save f1 metrics file\n",
    "    reports_folder = Path(config['evaluate']['reports_dir'])\n",
    "    \n",
    "    # Create the folder if it does not exist\n",
    "    os.makedirs(reports_folder, exist_ok=True)\n",
    "    \n",
    "    metrics_path = reports_folder / config['evaluate']['metrics_file']\n",
    "\n",
    "    json.dump(\n",
    "        obj={'f1_score': report['f1']},\n",
    "        fp=open(metrics_path, 'w')\n",
    "    )\n",
    "\n",
    "    logger.info(f'F1 metrics file saved to : {metrics_path}')\n",
    "\n",
    "    logger.info('Save confusion matrix')\n",
    "    # save confusion_matrix.png\n",
    "    plt = plot_confusion_matrix(cm=report['cm'],\n",
    "                                target_names=['Yes','No'],\n",
    "                                normalize=False)\n",
    "    confusion_matrix_png_path = reports_folder / config['evaluate']['confusion_matrix_image']\n",
    "    plt.savefig(confusion_matrix_png_path)\n",
    "    logger.info(f'Confusion matrix saved to : {confusion_matrix_png_path}')\n",
    "\n",
    "    confusion_matrix_data_path = reports_folder / config['evaluate']['confusion_matrix_data']\n",
    "    write_confusion_matrix_data(y_test, prediction, labels=labels, filename=confusion_matrix_data_path)\n",
    "    logger.info(f'Confusion matrix data saved to : {confusion_matrix_data_path}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    args_parser = argparse.ArgumentParser()\n",
    "    args_parser.add_argument('--config', dest='config', required=True)\n",
    "    args = args_parser.parse_args()\n",
    "\n",
    "    evaluate_model(config_path=args.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa68c64-4511-462d-802e-db8af9dc722b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779c8636-b37c-4985-8da7-8de24fe08963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9aa3579f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Training All shapes: (49865, 145)\n",
      "Training shapes: (11222, 145) (11222,)\n",
      "Validation shapes: (10685, 145) (10685,)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from train import train\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e1ab394b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shint\\.pyenv\\pyenv-win\\versions\\3.9.0\\python.exe\n",
      "D:\\IK_CAPSTONE_PROJECT\\medi-venv\\Scripts\\python.exe\n",
      "C:\\Users\\shint\\AppData\\Local\\Programs\\Python\\Python310\\python.exe\n",
      "C:\\Users\\shint\\AppData\\Local\\Microsoft\\WindowsApps\\python.exe\n"
     ]
    }
   ],
   "source": [
    "!where python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e0301e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
